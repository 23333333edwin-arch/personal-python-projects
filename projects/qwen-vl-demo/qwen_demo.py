from PIL import Image
from transformers import AutoProcessor, AutoModelForVision2Seq
import torch
import gradio as gr

# === åŠ è½½æ¨¡å‹ï¼ˆåªåŠ è½½ä¸€æ¬¡ï¼‰===
model_name = "Qwen/Qwen2.5-VL-3B-Instruct"

print("æ­£åœ¨åŠ è½½æ¨¡å‹ï¼Œè¯·ç¨å€™...")
processor = AutoProcessor.from_pretrained(model_name, trust_remote_code=True)
model = AutoModelForVision2Seq.from_pretrained(
    model_name,
    device_map="auto",
    trust_remote_code=True,
    torch_dtype=torch.float16
)
print("âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼")

# === é¢„è®¾ç³»ç»Ÿ Prompt æ¨¡æ¿åˆ—è¡¨ ===
SYSTEM_PROMPT_TEMPLATES = {
    "é€šç”¨åŠ©æ‰‹": "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„äººå·¥æ™ºèƒ½åŠ©æ‰‹ï¼Œèƒ½å¤Ÿå›ç­”é—®é¢˜ã€æè¿°å›¾åƒã€è¿›è¡Œæ¨ç†ç­‰ã€‚",
    "ç¿»è¯‘ä¸“å®¶": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç¿»è¯‘åŠ©æ‰‹ï¼Œè¯·å°†ç”¨æˆ·è¾“å…¥å‡†ç¡®ç¿»è¯‘æˆç›®æ ‡è¯­è¨€ï¼Œä¿æŒè¯­ä¹‰å’Œè¯­æ°”ä¸€è‡´ã€‚",
    "æ•°å­¦è€å¸ˆ": "ä½ æ˜¯ä¸€ä¸ªè€å¿ƒçš„æ•°å­¦è€å¸ˆï¼Œè¯·ç”¨æ¸…æ™°ã€åˆ†æ­¥çš„æ–¹å¼è§£é‡Šæ•°å­¦é—®é¢˜ï¼Œé€‚åˆä¸­å­¦ç”Ÿç†è§£ã€‚",
    "å›¾åƒåˆ†æå¸ˆ": "ä½ æ˜¯ä¸€ä¸ªå›¾åƒåˆ†æä¸“å®¶ï¼Œè¯·è¯¦ç»†æè¿°å›¾ç‰‡ä¸­çš„æ‰€æœ‰ç‰©ä½“ã€åœºæ™¯ã€é¢œè‰²ã€åŠ¨ä½œå’Œå¯èƒ½çš„ä¸Šä¸‹æ–‡ã€‚",
    "ç§‘æ™®ä½œå®¶": "ä½ æ˜¯ä¸€ä¸ªç§‘æ™®ä½œå®¶ï¼Œç”¨é€šä¿—æ˜“æ‡‚ã€ç”ŸåŠ¨æœ‰è¶£çš„è¯­è¨€è§£é‡Šå¤æ‚çš„ç§‘å­¦æ¦‚å¿µï¼Œé¿å…æœ¯è¯­ã€‚",
    "ç¼–ç¨‹åŠ©æ‰‹": "ä½ æ˜¯ä¸€ä¸ªèµ„æ·±ç¨‹åºå‘˜ï¼Œè¯·å¸®åŠ©ç”¨æˆ·ç¼–å†™ã€è°ƒè¯•æˆ–è§£é‡Šä»£ç ï¼Œæä¾›æœ€ä½³å®è·µå»ºè®®ã€‚",
    "åˆ›æ„å†™æ‰‹": "ä½ æ˜¯ä¸€ä¸ªåˆ›æ„å†™æ‰‹ï¼Œè¯·æ ¹æ®ç”¨æˆ·è¦æ±‚åˆ›ä½œæ•…äº‹ã€è¯—æ­Œã€å¹¿å‘Šæ–‡æ¡ˆæˆ–ç¤¾äº¤åª’ä½“å†…å®¹ï¼Œé£æ ¼è‡ªç”±ã€‚",
    "å†·é™AI": "ä½ æ˜¯ä¸€ä¸ªå†·é™ã€ç†æ€§ã€ä¸å¸¦æƒ…æ„Ÿçš„AIåŠ©æ‰‹ï¼Œåªæä¾›äº‹å®å’Œé€»è¾‘åˆ†æï¼Œä¸å®‰æ…°ã€ä¸é¼“åŠ±ã€‚"
}

# === æ¨ç†å‡½æ•° ===
def predict(image, user_prompt, system_prompt):
    # å¦‚æœç³»ç»Ÿ Prompt ä¸ºç©ºï¼Œä½¿ç”¨é»˜è®¤å€¼
    if not system_prompt.strip():
        system_prompt = SYSTEM_PROMPT_TEMPLATES["é€šç”¨åŠ©æ‰‹"]
    
    if image is None:
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]
        text_input = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        inputs = processor(
            text=[text_input],
            return_tensors="pt"
        ).to(model.device)
    else:
        image_pil = image.convert("RGB")
        image_pil.thumbnail((384, 384), Image.Resampling.LANCZOS)
        messages = [
            {"role": "system", "content": system_prompt},
            {
                "role": "user",
                "content": [
                    {"type": "image"},
                    {"type": "text", "text": user_prompt}
                ]
            }
        ]
        text_input = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        inputs = processor(
            images=image_pil,
            text=[text_input],
            return_tensors="pt"
        ).to(model.device)

    input_len = inputs["input_ids"].shape[1]  # â­ å…³é”®ï¼šè®°å½•è¾“å…¥é•¿åº¦

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=256,
            do_sample=True,
            temperature=0.7,
            top_p=0.9,
        )

    # â­ åªå–æ–°ç”Ÿæˆçš„éƒ¨åˆ†ï¼ˆè·³è¿‡è¾“å…¥ï¼‰
    generated_tokens = outputs[0][input_len:]
    response = processor.decode(generated_tokens, skip_special_tokens=True)
    return response.strip()

# === åº”ç”¨æ¨¡æ¿å‡½æ•°ï¼šç‚¹å‡»æŒ‰é’®åè‡ªåŠ¨å¡«å……ç³»ç»Ÿ Prompt ===
def apply_template(template_key):
    return SYSTEM_PROMPT_TEMPLATES.get(template_key, SYSTEM_PROMPT_TEMPLATES["é€šç”¨åŠ©æ‰‹"])

# === Gradio ç•Œé¢ ===
with gr.Blocks(title="Qwen-VL åŠ©æ‰‹") as demo:
    gr.Markdown("# ğŸ–¼ï¸ Qwen2.5-VL è§†è§‰è¯­è¨€åŠ©æ‰‹")
    gr.Markdown("ä¸Šä¼ ä¸€å¼ å›¾ç‰‡å¹¶æé—®ï¼Œæˆ–ç›´æ¥è¾“å…¥æ–‡æœ¬é—®é¢˜ã€‚é€‰æ‹©é¢„è®¾è§’è‰²ï¼Œä¸€é”®ä¼˜åŒ–æ¨¡å‹è¡Œä¸ºã€‚")
    
    with gr.Row():
        with gr.Column(scale=1):
            image_input = gr.Image(type="pil", label="ä¸Šä¼ å›¾ç‰‡ï¼ˆå¯é€‰ï¼‰")
        
        with gr.Column(scale=2):
            # ğŸ‘‡ æ–°å¢ï¼šè§’è‰²æ¨¡æ¿é€‰æ‹©å™¨ï¼ˆå•é€‰æŒ‰é’®ï¼‰
            template_radio = gr.Radio(
                choices=list(SYSTEM_PROMPT_TEMPLATES.keys()),
                value="é€šç”¨åŠ©æ‰‹",
                label="é€‰æ‹©è§’è‰²æ¨¡æ¿",
                interactive=True
            )
            
            # ğŸ‘‡ æ–°å¢ï¼šåº”ç”¨æ¨¡æ¿æŒ‰é’®
            apply_btn = gr.Button("âœ¨ åº”ç”¨æ¨¡æ¿", variant="primary")
            
            # ğŸ‘‡ ç³»ç»Ÿ Prompt è¾“å…¥æ¡†ï¼ˆç”±æŒ‰é’®è‡ªåŠ¨å¡«å……ï¼Œä¹Ÿå¯æ‰‹åŠ¨ä¿®æ”¹ï¼‰
            system_prompt_input = gr.Textbox(
                label="ç³»ç»Ÿ Promptï¼ˆå¯ç¼–è¾‘ï¼‰",
                placeholder="é€‰æ‹©æ¨¡æ¿åè‡ªåŠ¨å¡«å……ï¼Œä¹Ÿå¯æ‰‹åŠ¨ä¿®æ”¹...",
                value=SYSTEM_PROMPT_TEMPLATES["é€šç”¨åŠ©æ‰‹"],
                lines=4,
                max_lines=12
            )
            
            text_input = gr.Textbox(
                label="ä½ çš„é—®é¢˜", 
                placeholder="ä¾‹å¦‚ï¼šæè¿°è¿™å¼ å›¾ç‰‡", 
                lines=2
            )
            submit_btn = gr.Button("ğŸš€ æäº¤", variant="primary")
    
    output = gr.Textbox(
        label="æ¨¡å‹å›ç­”",
        interactive=False,
        lines=6,
        max_lines=30,
        placeholder="æ¨¡å‹çš„å›ç­”å°†æ˜¾ç¤ºåœ¨è¿™é‡Œ...",
        show_copy_button=True
    )

    # ğŸ‘‡ ç»‘å®šï¼šç‚¹å‡»â€œåº”ç”¨æ¨¡æ¿â€æŒ‰é’® â†’ å¡«å……ç³»ç»Ÿ Prompt
    apply_btn.click(
        fn=apply_template,
        inputs=template_radio,
        outputs=system_prompt_input
    )

    # ğŸ‘‡ ç»‘å®šï¼šæäº¤æŒ‰é’® â†’ æ‰§è¡Œæ¨ç†
    submit_btn.click(
        fn=predict,
        inputs=[image_input, text_input, system_prompt_input],
        outputs=output
    )

    # å¯é€‰ï¼šä¿ç•™ä¸€ä¸ªç®€å•ç¤ºä¾‹ï¼ˆä»…ä½œæ¼”ç¤ºï¼‰
    gr.Markdown("### ğŸ’¡ å°è´´å£«")
    gr.Markdown("- ä¸Šä¼ å›¾ç‰‡ + é€‰æ‹©ã€Œå›¾åƒåˆ†æå¸ˆã€â†’ è·å–è¯¦ç»†æè¿°\n- è¾“å…¥ä¸­æ–‡ + é€‰æ‹©ã€Œç¿»è¯‘ä¸“å®¶ã€â†’ è‡ªåŠ¨ç¿»è¯‘\n- æé—®æ•°å­¦é¢˜ + é€‰æ‹©ã€Œæ•°å­¦è€å¸ˆã€â†’ åˆ†æ­¥è®²è§£")

# å¯åŠ¨ï¼ˆå±€åŸŸç½‘å¯è®¿é—®ï¼‰
if __name__ == "__main__":
    demo.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=False
    )

# åœ¨æµè§ˆå™¨é€šè¿‡ http://localhost:7860 è®¿é—®æ­¤é¡¹ç›®
    